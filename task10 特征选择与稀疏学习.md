
## 子集搜索与评价
属性称为特征，对当前学习任务有用的属性称为相关特征，没用的属性称为无关特征。从给定的特征集合中选择出相关特征子集的过程称为特征选择。



**为什么进行特征选择**
1. 属性过多造成维数灾难
2. 去除不相关特征会降低学习任务的难度


**如何根据评价结果获取下一个候选特征子集**
![%E5%9B%BE%E7%89%87.png](attachment:%E5%9B%BE%E7%89%87.png)


**如何评价候选特征子集的好坏**
![%E5%9B%BE%E7%89%87.png](attachment:%E5%9B%BE%E7%89%87.png)

## 过滤式选择
过滤式方法先对数据集进行特征选择，然后再训练学习器，特征选择过程与后续学习器无关，这相当于先用特征选择过程对初始特征进行过滤，再用过滤后的特征来训练模型。

Relief是一种著名的过滤式特征选择方法，该方法设计了一个相关统计量来度量特征的重要性。该统计量是一个向量，其每个分量分别对应于一个初始特征，而特征子集的重要性则是由子集中每个特征所对应的相关统计量分量之和来决定。于是最终只需指定一个阈值，然后选择比该阈值大的相关统计量分量所对应的特征即可，也可指定欲选取的特征个数k，然后选择相关统计量分量最大的k个特征。

## 包裹式选择
包裹式特征选择直接把最终将要使用的学习器性能作为特征子集的评价准则，换言之，包裹式特征选择的目的就是为给定学习器选择最有利于其性能，量身定做的特征子集。

LVW包裹式特征选择方法
![%E5%9B%BE%E7%89%87.png](attachment:%E5%9B%BE%E7%89%87.png)

## 嵌入式选择于L1正则化
![%E5%9B%BE%E7%89%87.png](attachment:%E5%9B%BE%E7%89%87.png)
L1范数和L2范数正则化都有利于降低过拟合风险，但是前者还会带来一个额外的好处，它比后者更容易获得稀疏解，即它求得的w会有更少的非零分量。

## 稀疏表示与字典学习
为普通稠密表达的样本找到合适的字典，将样本转化为合适的稀疏表示形式，从而使学习任务得以简化，模型复杂度得以降低，通常称为字典学习，亦称稀疏编码。

## 压缩感知
![%E5%9B%BE%E7%89%87.png](attachment:%E5%9B%BE%E7%89%87.png)

与特征选择，稀疏表示不同，压缩感知关注的是如何利用信号本身所具有的稀疏性，从部分观测样本中恢复原信号。压缩感知分为感知测量和重构恢复。感知测量关注如何对原始信号进行处理以获得稀疏样本表示。重构恢复关注的是如何基于稀疏性从少量观测中恢复原信号。
