
# 决策树

## 基本流程

![%E5%9B%BE%E7%89%87.png](attachment:%E5%9B%BE%E7%89%87.png)

决策树的生成是第一个递归过程，有三种情形会导致递归返回：1.当前结点包含的样本全属于同一类别，无需划分。2.当前属性集为空，或是所有样本在属性上取值相同，无法划分。3.当前结点包含的样本集合为空，不能划分。

在第二种情形下，把当前结点标记为叶结点，并将其类别设定为该结点所含样本最多的类别；在第三种情形下，同样把当前结点标记为叶结点，但将其类别设定为其父结点所含样本最多的类别。

情形2是利用当前结点的后验分布，情形3是把父结点的样本分布作为当前结点的先验分布。

## 划分选择

### 信息增益
![%E5%9B%BE%E7%89%87.png](attachment:%E5%9B%BE%E7%89%87.png)

### 增益率

信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，C4.5决策树算法不直接使用信息增益，而是使用“增益率”来选择最优划分属性。信息增益率对可取值数目较少的属性有所偏好![%E5%9B%BE%E7%89%87.png](attachment:%E5%9B%BE%E7%89%87.png)

### 基尼指数

![%E5%9B%BE%E7%89%87.png](attachment:%E5%9B%BE%E7%89%87.png)

## 剪枝处理

剪枝是决策树学习算法对付“过拟合”的主要手段。

### 预剪枝

在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点。降低了过拟合的风险，还显著减少了决策树的训练时间开销和测试时间开销，但给预剪枝决策树带来了欠拟合的风险。

### 后剪枝
先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点。

## 连续与缺失值
### 连续值处理

最简单的策略是采用二分法对连续值进行处理。这正是C4.5决策树算法采用的机制。还有就是对连续数据进行离散化，分为几个值，然后当作离散值进行处理。

### 缺失值处理

这里存在两个问题：

如何在属性值缺失的情况下进行划分属性选择
给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分
对于第一个问题，若取值未知，则根据其他样本的取值来计算划分点。

对于第二个问题，若取值未知，则将该样本同时划入所有子结点，且设置一个样本权值用于计算loss。
