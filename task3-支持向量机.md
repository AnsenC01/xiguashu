
## 支持向量机定义

支持向量机（Support Vector Machine, SVM）是一类按监督学习（supervised learning）方式对数据进行二元分类（binary classification）的广义线性分类器（generalized linear classifier），其决策边界是对学习样本求解的最大边距超平面（maximum-margin hyperplane） 。
SVM使用铰链损失函数（hinge loss）计算经验风险（empirical risk）并在求解系统中加入了正则化项以优化结构风险（structural risk），是一个具有稀疏性和稳健性的分类器 。SVM可以通过核方法（kernel method）进行非线性分类，是常见的核学习（kernel learning）方法之一   。

**有监督学习**

线性二分类与多分类（Linear Support Vector Classification）

非线性二分类与多分类（Support Vector Classification, SVC）

普通连续型变量的回归（Support Vector Regression）

概率型连续变量的回归（Bayesian SVM）

**无监督学习**

支持向量聚类（Support Vector Clustering，SVC）
异常值检测（One-class SVM）

**半监督学习**

转导支持向量机（Transductive Support Vector Machines，TSVM）

从实际应用来看，SVM在各种实际问题中都表现非常优秀。它在手写识别数字和人脸识别中应用广泛，在文本和超
文本的分类中举足轻重，因为SVM可以大量减少标准归纳（standard inductive）和转换设置（transductive
settings）中对标记训练实例的需求。同时，SVM也被用来执行图像的分类，并用于图像分割系统。

## 2 支持向量机分类器是如何工作的
支持向量机所作的事情其实非常容易理解。先来看看下面这一组数据的分布，这是一组两种标签的数据，两种标签
分别由圆和方块代表。支持向量机的分类方法，是在这组分布中找出一个超平面作为决策边界，使模型在数据上的
分类误差尽量接近于小，尤其是在未知数据集上的分类误差（泛化误差）尽量小![%E5%9B%BE%E7%89%87.png](attachment:%E5%9B%BE%E7%89%87.png)

**超平面**：在几何中，超平面是一个空间的子空间，它是维度比所在空间小一维的空间。 如果数据空间本身是三维的，
则其超平面是二维平面，而如果数据空间本身是二维的，则其超平面是一维的直线。
在二分类问题中，如果一个超平面能够将数据划分为两个集合，其中每个集合中包含单独的一个类别，我们就
说这个超平面是数据的“决策边界”。


决策边界一侧的所有点在分类为属于一个类，而另一侧的所有点分类属于另一个类。如果我们能够找出决策边界，
分类问题就可以变成探讨每个样本对于决策边界而言的相对位置。比如上面的数据分布，我们很容易就可以在方块
和圆的中间画出一条线，并让所有落在直线左边的样本被分类为方块，在直线右边的样本被分类为圆。如果把数据
当作我们的训练集，只要直线的一边只有一种类型的数据，就没有分类错误，我们的训练误差就会为0。
但是，对于一个数据集来说，让训练误差为0的决策边界可以有无数条。![%E5%9B%BE%E7%89%87.png](attachment:%E5%9B%BE%E7%89%87.png)

但在此基础上，我们无法保证这条决策边界在未知数据集（测试集）上的表现也会优秀。对于现有的数据集来说，
我们有 和 两条可能的决策边界。我们可以把决策边界 向两边平移，直到碰到离这条决策边界最近的方块和
圆圈后停下，形成两个新的超平面，分别是 和 ，并且我们将原始的决策边界移动到 和 的中间，确保
到 和 的距离相等。在 和 中间的距离，叫做 这条决策边界的边际(margin)，通常记作 。对 也执行
同样的操作，然后我们来对比一下两个决策边界。现在两条决策边界右边的数据都被判断为圆，左边的数据都被判
断为方块，两条决策边界在现在的数据集上的训练误差都是0，没有一个样本被分错。![%E5%9B%BE%E7%89%87.png](attachment:%E5%9B%BE%E7%89%87.png)

我们引入和原本的数据集相同分布的测试样本（红色所示），平面中的样本变多了，此时我们可以发现，对于
而言，依然没有一个样本被分错，这条决策边界上的泛化误差也是0。但是对于 而言，却有三个方块被误人类成
了圆，二有两个圆被误分类成了方块，这条决策边界上的泛化误差就远远大于 了。这个例子表现出，拥有更大
边际的决策边界在分类中的泛化误差更小，这一点可以由结构风险最小化定律来证明（SRM）。如果边际很小，则
任何轻微扰动都会对决策边界的分类产生很大的影响。边际很小的情况，是一种模型在训练集上表现很好，却在测
试集上表现糟糕的情况，所以会“过拟合”。所以我们在找寻决策边界的时候，希望边际越大越好。![%E5%9B%BE%E7%89%87.png](attachment:%E5%9B%BE%E7%89%87.png)

**支持向量机，就是通过找出边际最大的决策边界，来对数据进行分类的分类器。**

sklearn中的svm
![%E5%9B%BE%E7%89%87.png](attachment:%E5%9B%BE%E7%89%87.png)
