
## 引言

    模型：指从数据中学得的结果
    模式：指局部性结果

### 基本术语

    数据集（data set）：一组记录的集合
    
    示例（instance）或样本（sample）：数据集中的一条记录，是关于一个事件或对象的描述。
    
    属性（attribute）或特征（feature）：反映对象某方面的表现或性质的事项。
    
    属性值（attribute value）：属性的具体取值。
    
    属性空间（attribute space）、样本空间（sample space）或输入空间：属性张成的空间。
    
    特征向量（feature vector）：属性空间中的每一个，向量点代表一个具体的对象。这个向量点就是特征向量
    
    训练数据（training data):从数据中学得模型的过程中使用的数据。这样每一个样本叫训练样本（training sample) 或训练示例（training instance)
    标记（label）：我们建立的预测。（是不是“好瓜”）
    
    分类（classification）：预测的是离散值。（如好瓜、坏瓜）对涉及两个类别的“二分类（binary classificaton）”任务,通常称一个叫“正类”（positive class）另一个叫反类（negative class）；涉及多分类任务时称为多分类（multi-class classification)
    
    回归（regression）：预测的是连续值。
    
    聚类（clustering）：对数据集进行分组，分组结果预先不知,学习过程中使用的训练样本通常不拥有标记信息
    
    监督学习（supervised learning）：有标记信息的学习任务，代表是分类和回归。
    
    非监督学习（unsupervised learning）：没有标记信息的学习任务，代表是聚类。
    
    泛化（generalization)能力：学得的模型适用新样本的能力

### 假设空间

    归纳（induction） :从特殊推到一般的泛化，从样例中学习就是归纳过程，归纳学习（inductive learning）
    
    演绎（deduction）：从一般到特殊
    
    版本空间：现实问题中我们常面临很大的假设空间，可学习过程是基于有限样本训练集进行的，因此可能有多个假设与训练集一致，即存在着一个与训练集一致的“假设集合”
    
    
### 归纳偏好

从一组数据中我们能学习出很多不同的模型。哪种模型更好这取决于我们如何给出”偏好“。


    奥卡姆剃刀准则:即有多个假设与观察一致，则选择最简单的那个。
    
    多释原则，主要保留与经验观察一致的所有假设。（与集成学习的思想非常吻合）。
    
    NFL定理（no free lunch theorem),证明误差与学习算法无关。但NFL定理建立在一个假设上：f（真实目标的函数）是均匀分布的(所有问题出现的机会相同或所有问题同等重要）。实际上，f并不是均匀分布的。根据我们对f的偏好来选择不同的模型。



### 基本形式

![%E5%9B%BE%E7%89%87.png](attachment:%E5%9B%BE%E7%89%87.png)

    回归分析(regression analysis)用来建立方程模拟两个或者多个变量之间如何关联
    
    被预测的变量叫做：因变量(dependent variable),被用来进行预测的变量叫做： 自变量(independentvariable),
    
    一元线性回归包含一个自变量和一个因变量以上两个变量的关系用一条直线来模拟如果包含两个以上的自变量，则称作多元回归分析(multiple regression)



### 线性回归

线性回归是监督学习中的重要算法，其主要目的在于用一个函数表示一组数据，其中横轴是变量（假定一个结果只由一个变量影响），纵轴是结果。线性回归得到的方程，称为假设函数（Hypothesis Function）。

    


    当Y值的影响因素不是唯一时(多个特征），采用多元线性回归模型。一元线性回归用一条直线表示，二元线性回归用平面表示，再往上这画不出来，用超平面拟合。
   
广义线性回归”： 我们不再只是用线性函数模拟数据， 而是在外层加了一个单调可微函数g， 即:f(x)=g−1(wx+b)。如果g=ln ， 则这个广义线性模型就变为对数线性回归。 其实本质就是给原来线性变换加上一个非线性变换(或者说映射)，使得模拟的函数有非线性的属性，但是， 本质上调参还是线性的，主体是内部线性的调参。



### 对数几率回归

”逻辑回归”也叫”对数回归” “，也叫”对数几率回归”。 其实， 这几个概念都是同一个概念。

事实上， “对数几率回归”不是解决回归问题的， 而是解决分类问题的。目的是要构造一个分类器Classifier. 并且, 关键不在于”回归”, 不在于如何用最大似然训练函数，也不在于用什么最优化方法训练函数。 关键在于”对数几率”这四个字， 在于对数几率函数。

为了解决一个最简单的二类分类问题， 我们为每一个点定义一个值域[0, 1]的函数，表示这个点分在A类或者B类中的可能性，如果非常可能是A类，那可能性就逼近1，如果非常可能是B类， 那可能性就逼近0(相对A的可能性)，如果两个类很难判别，当然就是0.5。于是构造出对数几率函数：
![%E5%9B%BE%E7%89%87.png](attachment:%E5%9B%BE%E7%89%87.png)



### 线性判别分析

寻找一条直线，将正例和反例的样本点分别投影到这条直线上，使得同类样本的投影距离尽可能近；不同类样本的投影距离尽可能远。

![%E5%9B%BE%E7%89%87.png](attachment:%E5%9B%BE%E7%89%87.png)

### 多分类学习

很多二分类模型可以直接推广到多分类的情况，但是更多情形下，我们是利用二分类器解决多分类问题，这一过程涉及到对多分类任务进行拆分，以及对多分类器进行集成。本节主要介绍了拆分策略：

OvO，就是“一对一”。模型在训练时挑选一个类别作为正类，一个类别作为负类，共N个类别时，需要训练N(N-1)/2个分类器。测试阶段，将测试用例喂给每个分类器，在预测结果中选择出现频次最多的作为最终的结果，相当于“投票法”，谁的票数多谁就当选。这种方法的优点是训练时不必用到所有的输入样例，而只需两个类别的样例即可，这在训练集十分庞大时有一定优势。但是它需要训练N²量级个分类器，这导致存储和测试时间的开销比较大。

OvR，就是“一对其余”。模型在训练时挑选一个类别作为正类，其余类别全部作为负类，共N个类别时，需要训练N个分类器。测试阶段，将测试用例喂给每个分类器，这时输出情况有两种：（1）只有唯一一个分类器输出为正类，那么此类就是最终的预测结果；（2）有多个分类器输出为正类，则考虑预测置信度，选择预测置信度大的分类作为最终输出。这种方法由于每次要用到全部训练数据，因此训练时间开销较大。

MvM，就是“多对多”。模型在训练时挑选若干个类别作为正类，若干个类别作为负类。一种常用的MvM技术叫做ECOC（Error Correcting Output Code）纠错输出码。具体做法是，（编码过程）将N个类别划分M次，在训练集上训练出来M个分类器。也就是说这M次过程之后，每个类别都有一个长度为M的编码，要么0（被划为负类），要么1（被划为正类）；（解码过程）将测试样例分别喂给这M个分类器，得到M个预测标记，组成长度为M的编码，依次计算此编码与每个类别编码的距离，距离最短的作为其类别。之所以叫做“纠错输出码”，是因为这种方法具有一定的容错性能。即使在解码过程中某个分类器出现了错误，依然能产生正确的分类结果。

### 类别不平衡问题

所谓类别不平衡问题是指训练集内正类数目和负类数目相差悬殊，如正例有998个，而负例只有2个。这样一来一个将所有输入全部预测为正例的无脑分类器也会达到很高的分类准确率，但是却毫无意义。

针对这一问题，有3种解决办法。以下假设正例个数远远大于负例个数。

    欠采样。随机抛弃一些正例，使二者数目相当。

    过采样。增加一些负例，使二者数目相当。如通过对训练集中的负例进行插值法以产生额外的负例。

    阈值转移法。以对数几率回归为例，y/1-y可以看作一种预测几率，即正例可能性和负例可能性的比值。当此几率大于1时（y>0.5）就预测为正例，反之为负例。现在将m+/m-定义为观测几率，m+为训练集中正例数目，m-为负例数目。当训练集是真实样本的无偏估计时（很遗憾，这种情况并不容易达到），观测几率就代表了真实几率。

