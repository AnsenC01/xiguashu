
### 个体与集成
集成学习的一般结构:先产生一组"个体学习 器" (individual learner)，再用某种策略将它们结合起来。同质集成中只包含同种类型的个体学习器，例如"决策树集成" 中全是决策树，“神经网络集成"中全是神经网络。同质集成中的个体学习器亦称基学习器 , 相应的学习算法称为基学习算法。 异质集成包含不同类型的个体学习器，例如同时包含决策树和神经网络，异质集成中的个体学习器由不同的学习算法生成。个体学习器一般不称为基学习器，常称为"组件学习器” 。
集成学习通过将多个学习器进行结合，常可获得比单一学习器显著优越的泛化性能。这对弱学习器尤为明显。（弱学习器：泛化性能略优于随机猜想的学习器。）
虽然从理论上来说使用弱学习器集成足以获得好的性能，但在实践中出于种种考虑，例如希望使用较少的个体学习器，或是重用关于常见学习器的一些经验等，人们往往会使用比较强的学习器。
要获得好的集成， 个体学习器应"好而不同”，即个体学习器要有一定的准确性。并且要有"多样性" ，即学习器间具有差异。	
![%E5%9B%BE%E7%89%87.png](attachment:%E5%9B%BE%E7%89%87.png)

### Boosting
Boosting 是一族可将弱学习器提升为强学习器的算法，这族算法的工作机制类似：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注， 然后基于调整后的样本分布来训练下一个基学习器;如此重复进行，直至基学习器数目达到事先指定的值 T， 最终将这 T 个基学习器进行加权结合。Boosting 族算法最著名的代表是 AdaBoost 。
AdaBoost 算法有多种推导方式，比较容易理解的是基于"加性模型" (additive model)，即基学习器的线性组合
![%E5%9B%BE%E7%89%87.png](attachment:%E5%9B%BE%E7%89%87.png)

从偏差一方差分解的角度看， Boosting 主要关住降低偏差，因此 Boosting 能基于泛化性能相当弱的学习器构建出很强的集成.

### Bagging与随机森林
欲得到泛化性能强的集成，集成中的个体学习器应尽可能相互独立；虽然"独立"在现实任务中无法做到，但可以设法使基学习器尽可能具有较大的差异。给定一个训练数据集，一种可能的做法是对训练样本进行采样，产生出若干个不同的子集，再从每个数据子集中训练出一个基学习器.这 样，由于训练数据不同，我们获得的基学习器可能具有比较大的差异。然而，为获得好的集成，我们同时还希望个体学习器不能太差。如果采样出的每个子集都完全不同，则每个基学习器只用到了一小部分训练数据，甚至不足以进行有效学习，这显然无法确保产生出比较好的基学习器。为解决这个问题，我们可考 虑使用相互有交叠的采样子集。
3.1 Bagging
Bagging是并行式集成学习方法，基于自助来样法 (bootstrap sampling)。给定包含 m 个样本的数据集，我们先随机取出一个样本放入采样集中，再把该样本放回初始数集，使得下次采样时该样本仍有可能被选中，这样，经过 m 次随机采样操作，我们得到含 m 个样本的采样集，初始训练集中有的样本在采样集里多次出现，有的则从未出现。由式(2.1)可知，初始训练集中约有 63.2% 的样本出现在来样集中。
照这样，我们可采样出 T 个含 m 个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合。这就是 Bagging 的基本流程。
在对预测输出进行结合时， Bagging 通常对分类任务使用简单投票法，对回归任务使用简单平均法。若分类预测时出现两个类收到同样票数的情形，则最简单的做法是随机选择一个，也可进一步考察学习器投票的置信度来确定最终胜者。
![%E5%9B%BE%E7%89%87.png](attachment:%E5%9B%BE%E7%89%87.png)

假定基学习器的计算复杂度为 O(m)， 则 Bagging的复杂度大致为 T (O (m) + O(s))，考虑到采样与投票/平均过程的复杂度O(s)很小，而 T 通常是一个不太大的常数，因此，训练一个 Bagging 集成与直接使用基学习器的复杂度同阶，这说明 Bagging 是一个很高效的集成学习算法。与标准 AdaBoost 只适用于二分类任务不间， Bagging 能不经修改地用于多分类、回归等任务。
由于每个 基学习器只使用了初始训练集中约 63.2% 的样本，剩下约 36.8% 的样本可用作验证集来对泛化性能进行包外估计。包外样本还有许多其他用途。例如当基学习器是决策树时，可使用包外样本来辅助剪枝。或用于估计决策树中各结点的后验概率以辅助对零训练样本结点的处理；当基学习器是神经网络时，可使用包外样本来辅助早期停止以减小过拟合风险。
从偏差方差分解的角度看， Bagging 主要关注降低方差，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效用更为明显。


#### 3.2 随机森林
随机森林(Random Forest，简称 RF)是Bagging 的一个 扩展变体。RF在以决策树为基学习器构建 Bagging 集成的基础上，进一步在决策树的训练过程中引入了随机属性选择.具体来说，传统决策树在选择划分属性时是在当前结点的属性集合中选择一个最优属性；而在 RF 中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含 k个属性的子集，然后再从这个子集中选择一个最优属性用于划分。这里的参数 k 控制了随机性的引入程度；若令 k = d， 则基决策树的构建与传统决策树相同；若令 k = 1， 则是随机选择一个属性用于划分； 一般情况下，推荐值 k = log2 d
![%E5%9B%BE%E7%89%87.png](attachment:%E5%9B%BE%E7%89%87.png)

### 结合策略
![%E5%9B%BE%E7%89%87.png](attachment:%E5%9B%BE%E7%89%87.png)

投票法

绝对多数投票法(majority voting)：即若某标记得票过半数，则预测为该标记;否则拒绝预测。

相对多数投票法(plurality voting)：即预测为得票最多的标记，若同时有多个标记获最高票，则从中随机选取一个。
加权投票法(weighted yoting)：与加权平均法类似， Wi 是Xi的权重。

 学习法

多样性增强
